# Image setting
input_image_embed_size: 768

# Text Setting
vocab_size: 50265
mlm_prob: 0.15
#tokenizer = "roberta-base"
input_text_embed_size: 768

# Transformer Setting
hidden_size: 768
num_heads: 12
num_layers: 12
mlp_ratio: 4
drop_rate: 0.1
num_fuse_block: 6


# Gradient Checkpoint
use_checkpoint: True


# lr_scheduler
decay_power: 1
end_lr: 0
warmup_steps: 0.1 # This is a floating point indicating % of max_steps


# summarization

max_segment_num: 20
max_frame_num: 200
               
num_workers: 0
epoch: 20
batch_size: 4
lr: 0.0001
top_percent: 0.02

learning_rate_weights: 0.2
learning_rate_biases: 0.0048


#model_summary
model_dim: 768
nhead: 2
num_layers: 2
